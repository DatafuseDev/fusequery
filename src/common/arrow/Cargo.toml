[package]
description = "Arrow implementation forked from arrow2 and native format implementation forked from strawboat."
edition = "2021"
license = "Apache-2.0"
name = "databend-common-arrow"
publish = false
version = "0.1.0"

[lib]
doctest = false
test = true

[features]
default = ["arrow-default", "parquet-default"]

arrow = ["arrow-buffer", "arrow-schema", "arrow-data", "arrow-array"]
io_flight = ["io_ipc", "arrow-format/flight-data"]
io_ipc = []
io_ipc_compression = []

# base64 + io_ipc because arrow schemas are stored as base64-encoded ipc format.
io_parquet = ["io_ipc", "base64", "streaming-iterator", "fallible-streaming-iterator"]
io_parquet_async = ["futures", "io_parquet", "parquet2/async"]

io_parquet_compression = [
    "io_parquet_zstd",
    "io_parquet_gzip",
    "io_parquet_snappy",
    "io_parquet_lz4",
    "io_parquet_brotli",
]

# sample testing of generated arrow data
io_parquet_sample_test = ["io_parquet_async"]

# compression backends
io_parquet_brotli = ["parquet2/brotli"]
io_parquet_gzip = ["parquet2/gzip"]
io_parquet_lz4 = ["parquet2/lz4"]
io_parquet_snappy = ["parquet2/snappy"]
io_parquet_zstd = ["parquet2/zstd"]

# parquet bloom filter functions
io_parquet_bloom_filter = ["parquet2/bloom_filter"]

compute = [
    "compute_aggregate",
    "compute_cast",
    "compute_concatenate",
    "compute_merge_sort",
    "compute_sort",
    "compute_take",
]
compute_aggregate = []
compute_cast = ["lexical-core", "compute_take"]
compute_concatenate = []
compute_merge_sort = ["itertools", "compute_sort"]
compute_sort = ["compute_take"]
compute_take = []

serde_types = ["serde", "serde_derive"]
simd = []

arrow-default = [
    "arrow",
    "io_ipc",
    "io_ipc_compression",
    "io_flight",
    "io_parquet_async",
    "io_parquet_compression",
    "io_parquet",
    "compute",
    "serde_types",
    "simd",
]

parquet-default = [
    "parquet2/lz4",
    "parquet2/zstd",
    "parquet2/snappy",
    # this feature can't be built in musl
    # "parquet2/gzip_zlib_ng",
    "parquet2/brotli",
]

[dependencies]
ahash = { workspace = true }
arrow-array = { workspace = true, optional = true }
arrow-buffer = { workspace = true, optional = true }
arrow-data = { workspace = true, optional = true }
arrow-format = { workspace = true }
arrow-schema = { workspace = true, optional = true }
async-stream = { workspace = true, optional = true }
base64 = { workspace = true, optional = true }
bitpacking = { workspace = true }
bytemuck = { workspace = true }
byteorder = { workspace = true }
bytes = { workspace = true }
chrono = { workspace = true }
chrono-tz = { workspace = true, optional = true }
dyn-clone = { workspace = true }
either = { workspace = true }
ethnum = { workspace = true }
fallible-streaming-iterator = { workspace = true, optional = true }
foreign_vec = { workspace = true }
futures = { workspace = true, optional = true }
hashbrown_v0_14 = { workspace = true }
indexmap = { workspace = true }
itertools = { workspace = true, optional = true }
lexical-core = { workspace = true, optional = true }
log = { workspace = true }
lz4 = { workspace = true }
num = { workspace = true, features = ["std"] }
num-traits = { workspace = true }
opendal = { workspace = true }
ordered-float = { workspace = true }
parquet2 = { workspace = true }
rand = { workspace = true }
ringbuffer = { workspace = true }
roaring = { workspace = true }
serde = { workspace = true, features = ["rc"], optional = true }
serde_derive = { workspace = true, optional = true }
simdutf8 = { workspace = true }
snap = { workspace = true }
streaming-iterator = { workspace = true, optional = true }
zstd = { workspace = true }

[dev-dependencies]
# used to test async readers
tokio = { workspace = true, features = ["macros", "rt", "fs", "io-util"] }
tokio-util = { workspace = true, features = ["compat"] }
env_logger = { workspace = true }
flate2 = { workspace = true }
proptest = { workspace = true, default-features = false, features = ["std"] }
quanta = { workspace = true }

[lints]
workspace = true
